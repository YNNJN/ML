{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uy4KaKmU2fbn"
   },
   "source": [
    "# 04-6 로지스틱 회귀 뉴런으로 단일층 신경망 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qn5dzWPo2fbp"
   },
   "source": [
    "#### 일반적인 신경망은 다음과 같이 입력층과 출력층 사이에 은닉층이 있으며 활성화 함수를 은닉층과 출력층의 한 부분으로 간주한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YzJwmxh2fbr"
   },
   "source": [
    "![neural network](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbFKOXR%2FbtqCjdQQuEk%2FMn8e05EkdzPZT49nS98RD0%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeE0V6ga2fbt"
   },
   "source": [
    "### 로지스틱 회귀 = 단일층 신경망(single layer neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-L_DIO_2fbv"
   },
   "source": [
    "- 은닉층이 없는 신경망\n",
    "    - 입력층 : 입력 그 자체이므로 프로그램 구현 시 겉으로 드러나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brCYUpEJ2fbw"
   },
   "source": [
    "![single layer neural network](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FNVHpi%2FbtqCpPU3cF6%2FsmO07gw1KrgHKqRwPjGTeK%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ful4CLhJ2fb0"
   },
   "source": [
    "### 지난주 시간에 구현한 LogisticNeuron class에 몇 가지 기능을 추가하여 단일층 신경망을 구현하기\n",
    "- 손실 함수(제곱 오차 손실 함수, 로지스틱 손실 함수)의 결괏값을 최소화하는 방향으로 가중치 업데이트했었다.\n",
    "    - 만약 손실 함수의 결괏값이 줄어들지 않는다면? ==> 잘못된 것\n",
    "    - ∴ 손실 함수의 결괏값을 관찰하는 가능 등을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "a4flvUIt2fb2",
    "outputId": "f525727b-bc56-413e-ac28-52f8650e90fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "x = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2, random_state = 42)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpyJniSy2fb9"
   },
   "source": [
    "### 손실 함수의 결괏값 조정해 저장 기능 추가하기\n",
    "\n",
    "#### __init__() 메서드에 손실 함수의 결괏값을 저장할 리스트 self.losses를 만든 후, 샘플마다 손실 함수를 계산하고 그 결괏값을 모두 더한 다음 평균값을 self.losses 변수에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t56WK9vF2fcA"
   },
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "    self.losses = []\n",
    "    \n",
    "# 다른 메서드 생략\n",
    "\n",
    "def fit(self, x, y, epochs=100):\n",
    "        self.w = np.ones(x.shape[1]) # 가중치 초기화\n",
    "        self.b = 0                   # 절편 초기화\n",
    "        for i in range(epochs):      # epochs만큼 반복\n",
    "            loss = 0\n",
    "            indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 무작위로 섞는다.\n",
    "            # np.random.permutation ==> 배열이 저장된 변수자체를 변경하는것 X, 랜덤으로 섞은 배열만을 다시 반환\n",
    "            for i in indexes:        # 모든 샘플에 대해 반복\n",
    "                z = self.forpass(x[i])  # 정방향 계산\n",
    "                a = self.activation(z) # 활성화 함수 적용\n",
    "                err = -(y[i] - a)     # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
    "                self.w -= w_grad     # 가중치 업데이트\n",
    "                self.b -= b_grad     # 절편 업데이트\n",
    "                # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적한다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a)) # epoch마다 평균 손실을 저장\n",
    "            self.losses.append(loss/len(y)) # 모두 더한 손실 함수 결괏값을 샘플 개수로 나눈 평균값을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmfCd7Kf2fcF"
   },
   "source": [
    "#### self.activation() 메서드로 계산한 a는 np.log()의 계산을 위해 한 번 더 조정한다. 왜?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTwNh8s-2fcG"
   },
   "source": [
    "- a가 0에 가까워질 때, np.log() 함수의 값 ==> -∞\n",
    "- a가 1에 가까워질 때, np.log() 함수의 값 ==> 0\n",
    "    - ∴ 손실값이 무한해지면 정확한 계산을 할 수 없기때문에 a값이 -1*10^-10 ~ 1-1*10^10 사이가 되도록 조정\n",
    "        - 이 때 np.clip(배열, 최소값 기준, 최대값 기준) 함수 사용해서 지정 범위 기준을 벗어나는 값에 대해 일괄적으로 최소값과 최대값을 대치해준다. 즉, 주어진 범위 밖의 값을 범위 양 끝의 값으로 잘라낸다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jMf2eiW2fcI"
   },
   "source": [
    "### 여러 가지 경사 하강법(gradient descent)에 대해 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V90qq572fcJ"
   },
   "source": [
    "#### 1. 확률적 경사 하강법(SGD, stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1SipYqM2fcL"
   },
   "source": [
    "- 무작위로 추출한 일부 샘플 데이터(전체 데이터 사용X)를 각각 1개씩 그레디언트를 계산하여 가중치 업데이트\n",
    "- 속도가 매우 빠르고 계산 비용이 적다.\n",
    "- 샘플 선택이 확률적이기 때문에 배치 경사 하강법에 비해 불안정하다.\n",
    "- 가중치가 최적값에 수렴하는 학습 중간 과정에서 결과의 진폭이 크고 불안정하다.\n",
    "- 데이터를 1개씩 처리하기 때문에 오차율이 크고 GPU의 성능을 모두 활용하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8vFrc_c2fcM"
   },
   "source": [
    "![sgd](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FoctWe%2FbtqClyGKYFU%2FeL2f8aTZnNKPIjhXdAjDz1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPrfS_ov2fcO"
   },
   "source": [
    "#### 2. 배치 경사 하강법(BGD, batch gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56kYuHoT2fcP"
   },
   "source": [
    "- 전체 훈련 데이터 세트를 사용하여 한 번에 그레디언트를 계산\n",
    "- 가중치가 최적값에 수렴하는 과정이 안정적이다.\n",
    "- 데이터가 많을 경우 수행 시간이 오래 걸리고 계산 비용이 많이 든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvOuUftA2fcQ"
   },
   "source": [
    "#### 3. 미니 배치 경사 하강법(mini-batch gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzhRS6TY2fcS"
   },
   "source": [
    "- 확률적 경사 하강법의 장점 + 배치 경사 하강법의 장점\n",
    "- 중복되지 않도록 무작위로 추출한 훈련 세트를 여러 번 나누어 배치 크기를 작게 한 후 그레디언트 계산\n",
    "- SGD(확률적 경사 하강법)에 비해 병렬 처리에 유리하며 파라미터 공간에서 덜 불규칙하게 학습\n",
    "- BGD(배치 경사 하강법)보다 빠르고 SGD보다 낮은 오차율을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW80rDiQ2fcU"
   },
   "source": [
    "![mini-batch](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FqeOqW%2FbtqCpP1PGmG%2Fatwt2hdxaWzCiGKVpDJV80%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ar3sDHgT2fcY"
   },
   "source": [
    "### 3가지 경사 하강법 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLZtqJgo2fcZ"
   },
   "source": [
    "![3gd](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fbgycut%2FbtqCjRthlC1%2Fbai13TwOioBKUB2uveySCK%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrv0E6NL2fca"
   },
   "source": [
    "- 모두 최소값 근처에 도달했지만 배치 경사 하강법의 경로가 실제로 최소값에서 멈춘 반면 확률적 경사 하강법과 미니 배치 경사 하강법은 근처에서 맴돌고 있는데 배치 경사 하강법은 매 스텝에서 많은 시간이 소요되며 확률적 경사 하강법, 미니 배치 경사 하강법은 적절한 학습 스케쥴을 사용하면 최소값에 도달할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nlMHM_x2fcc"
   },
   "source": [
    "### 매 에포크마다 훈련 세트의 샘플 순서를 섞어 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPaKHtit2fcd"
   },
   "source": [
    "- 훈련 세트의 샘플 순서를 섞으면 가중치 최적값의 탐색 과정이 다양해져 제대로 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VA0en7A2fcf"
   },
   "source": [
    "![epochs](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FdoeBrp%2FbtqCn1hmMvL%2FJRM2iRl2gKYWdSuRymnWj1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQnQoCWT2fcg"
   },
   "source": [
    "|<center>1번째 에포크 샘플 순서</center>|<center>2번째 에포크 샘플 순서</center>|\n",
    "|:-------------|:-------------:|\n",
    "|<center>**1, 3, 2**</center>|<center>**1, 2, 4**</center>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT5jDIHH2fch"
   },
   "source": [
    "- 훈련 세트의 샘플 순서를 섞는 전형적인 방법은 Numpy 배열의 인덱스를 섞은 후 인덱스 순서대로 훈련 세트를 나열하는 것\n",
    "    - 훈련 세트 자체를 섞는 것보다 효율적이고 빠르다. ==> np.random.permutation() 함수 사용 시 구현가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAwgwxBL2fci"
   },
   "outputs": [],
   "source": [
    "def fit(self, x, y, epochs=100):\n",
    "        self.w = np.ones(x.shape[1]) # 가중치 초기화\n",
    "        self.b = 0                   # 절편 초기화\n",
    "        for i in range(epochs):      # epochs만큼 반복\n",
    "            loss = 0\n",
    "            indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 무작위로 섞는다.\n",
    "            # np.random.permutation ==> 배열이 저장된 변수자체를 변경하는것 X, 랜덤으로 섞은 배열만을 다시 반환\n",
    "            for i in indexes:        # 모든 샘플에 대해 반복\n",
    "                z = self.forpass(x[i])  # 정방향 계산\n",
    "                a = self.activation(z) # 활성화 함수 적용\n",
    "                err = -(y[i] - a)     # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
    "                self.w -= w_grad     # 가중치 업데이트\n",
    "                self.b -= b_grad     # 절편 업데이트\n",
    "                # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적한다.\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a)) # epoch마다 평균 손실을 저장\n",
    "            self.losses.append(loss/len(y)) # 모두 더한 손실 함수 결괏값을 샘플 개수로 나눈 평균값을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WkCQtSQr2fcm"
   },
   "source": [
    "- 정방향과 오차를 계산할 때 indexes 배열의 인덱스(x[i], y[i])를 사용하여 샘플을 참조한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHbzvyIM2fcn"
   },
   "source": [
    "### 정확도 계산해 주는 score() 메서드 추가 및 predict() 메서드 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzaLnn-32fco"
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    z = [self.forpass(x_i) for x_i in x] # 정방향 계산\n",
    "    # a = self.activation(np.array(z))\n",
    "    # 시그모이드 함수의 출력값은 0~1 사이의 확률값이고 양성 클래스를 판단하는 기준 ==> 0.5 이상\n",
    "    # z가 0보다 크면? 시그모이드 함수의 출력값 > 0.5\n",
    "    # z가 0보다 작으면? 시그모이드 함수의 출력값 < 0.5\n",
    "    # 따라서 z가 0보다 큰지, 작은지만 판단하면 되므로 해당 메서드에서는 굳이 시그모이드 함수를 사용하지 않는다.\n",
    "    return np.array(z) > 0               # 계단 함수 적용\n",
    "\n",
    "def score(self, x, y):\n",
    "    return np.mean(self.predict(x) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxvzAkj52fcr"
   },
   "source": [
    "### 단일층 신경망 클래스의 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNWXrXXm2fct"
   },
   "outputs": [],
   "source": [
    "class SingleLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.losses = []\n",
    "    \n",
    "    def forpass(self, x):\n",
    "        z = np.sum(x * self.w) + self.b # 직선 방정식 계산\n",
    "        return z\n",
    "    \n",
    "    def backprop(self, x, err):\n",
    "        w_grad = x * err # 가중치에 대한 그레디언트 계산\n",
    "        b_grad = 1 * err # 절편에 대한 그레디언트 계산\n",
    "        return w_grad, b_grad\n",
    "    \n",
    "    def add_bias(self, x):\n",
    "        return np.c_[np.ones((x.shape[0], 1), x)] # 행렬의 맨 앞에 1로 채워진 열 벡터를 추가\n",
    "    \n",
    "    def activation(self, z):\n",
    "        a = 1 / (1 + np.exp(-z)) # 시그모이드 계산\n",
    "        return a\n",
    "    \n",
    "    def fit(self, x, y, epochs=100):\n",
    "        self.w = np.ones(x.shape[1]) # 가중치 초기화\n",
    "        self.b = 0                   # 절편 초기화\n",
    "        for i in range(epochs):      # epochs만큼 반복\n",
    "            loss = 0\n",
    "            indexes = np.random.permutation(np.arange(len(x))) # 인덱스를 무작위로 섞는다.\n",
    "            for i in indexes:        # 모든 샘플에 대해 반복\n",
    "                z = self.forpass(x[i])  # 정방향 계산\n",
    "                a = self.activation(z) # 활성화 함수 적용\n",
    "                err = -(y[i] - a)     # 오차 계산\n",
    "                w_grad, b_grad = self.backprop(x[i], err) # 역방향 계산\n",
    "                self.w -= w_grad     # 가중치 업데이트\n",
    "                self.b -= b_grad     # 절편 업데이트\n",
    "                a = np.clip(a, 1e-10, 1-1e-10)\n",
    "                loss += -(y[i] * np.log(a) + (1-y[i]) * np.log(1-a)) # epoch마다 평균 손실을 저장\n",
    "            self.losses.append(loss/len(y)) # 모두 더한 손실 함수 결괏값을 샘플 개수로 나눈 평균값을 저장\n",
    "            \n",
    "    def predict(self, x):\n",
    "        z = [self.forpass(x_i) for x_i in x] # 정방향 계산\n",
    "        return np.array(z) > 0               # 계단 함수 적용\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp_vd3hc2fcy"
   },
   "source": [
    "### 단일층 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rp9zSS82fcz"
   },
   "source": [
    "#### 1. 단일층 신경망 훈련하고 정확도 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "ZfDfd75B2fc1",
    "outputId": "bb44b77f-0e8e-400b-e2fa-8e38b6fd7b4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9385964912280702"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = SingleLayer() # LogisticNeuron과 마찬가지로 SingleLayer 객체 생성\n",
    "layer.fit(x_train, y_train) # 신경망 훈련\n",
    "layer.score(x_test, y_test) # 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6eHpE5s2fc3"
   },
   "source": [
    "### LogisticNeuron과 동일하게 fit() 메서드의 에포크 매개변수 기본값을 사용했는데 왜 성능이 좋아졌나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qZTp1aF2fc4"
   },
   "source": [
    "- 에포크마다 훈련 세트를 무작위로 섞어 손실 함수의 값을 줄였기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE97x2_A2fc6"
   },
   "source": [
    "#### 2. 손실 함수 누적값 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRuqVw_d2fc7"
   },
   "source": [
    "- layer 객체의 losses 속성에 append한 값(모두 더한 손실 함수 결괏값을 샘플 개수로 나눈 평균값들)을 그래프로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "42LyLJBw2fc7",
    "outputId": "44268730-caf7-42b9-f99a-5716e43cac2f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNMArpVSc0gCvlFJxSgO8UkrFqf8PG7f0nYl2QscAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(layer.losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cn8jCvF62fc9"
   },
   "source": [
    "## 가장 기초적인 신경망 알고리즘 구현 완성!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDdt0EjM2fc-"
   },
   "source": [
    "- 로지스틱 손실 함수의 값이 에포크가 진행됨에 따라 감소하고 있음을 확인할 수 있다.\n",
    "- 신경망 알고리즘은 로지스틱 회귀 알고리즘을 확장한 네트워크로 생각해도 좋다.\n",
    "    - 단, 은닉층을 사용하지 않았으므로 이 단일층 신경망은 로지스틱 회귀나 퍼셉트론 알고리즘과 매우 비슷하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rVaTV7u2fc_"
   },
   "source": [
    "# 사이킷런(Scikit-learn)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ov3EJEcF2fdA"
   },
   "source": [
    "- 파이썬(Python)의 머신러닝 라이브러리\n",
    "- 머신러닝 기술을 활용하는데 필요한 다양한 기능을 제공\n",
    "    - 기계 학습(Machine learning)은 인공 지능의 한 분야, 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야\n",
    "- 딥러닝 모델을 텐서플로, 케라스, 파이토치를 이용해서 생성 할 수 있는 것처럼 머신러닝은 주로 사이킷런을 사용\n",
    "- 지도학습을 위한 모듈, 비지도학습을 위한 모듈, 모델 선택 및 평가를 위한 모듈, 데이터 변환 및 데이터를 불러오기 위한 모듈, 계산 성능 향상을 위한 모듈로 구성돼 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxbV0_mS2fdB"
   },
   "source": [
    "### 사이킷런의 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNyxtRQM2fdC"
   },
   "source": [
    "#### 1. 지도 학습 모듈 : 나이브 베이즈, 의사결정 트리, 서포트 벡터 머신\n",
    "\n",
    "#### 2. 비지도 학습 모듈 : 군집화, 가우시안 혼합 모델\n",
    "\n",
    "#### 3. 모델 선택과 평가 모듈 : 교차 검증, 모델 평가, 모델을 지속성을 위한 모델 저장과 불러오기\n",
    "\n",
    "#### 4. 데이터 변환 모듈 : 파이프라인, 특징 추출, 데이터 전처리, 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNbvNWAf2fdC"
   },
   "source": [
    "![sklearn algorithm cheat sheet](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fb3IBco%2FbtqCn1Ivt18%2FjR0DSK4MCeD9hVTYYbSVE1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bX2nKRTN2fdD"
   },
   "source": [
    "### 사이킷런 알고리즘 치트 시트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kttO9rar2fdE"
   },
   "source": [
    "- 사이킷런의 코어 개발자, 뉴욕 대학교의 리서치 엔지니어인 안드리아스 뮐러(Andreas C. Müller)가 만듦\n",
    "- 회귀분석(Regression), 분류(Classification), 군집(Clustering), 차원감소(Dimensionality Reduction)으로 구분\n",
    "    - 회귀분석(Regression) : 연속된 수치 값을 예측하는 데 사용되는 알고리즘\n",
    "        - 오래전 통계학자들에 의해 개발되고 발전되어 온 알고리즘으로 한 백년 전 즈음에 프란시스 갈톤(Francis Galton)이라는 통계학자가 아버지와 자식들의 키의 관계를 조사하다가 아버지의 키가 큰데 자식들의 키는 아버지보다 작아지는 경향이 있다는 것을 발견했다. 그래서 자식들의 키가 평균으로 회귀(regression)한다고 표현했고 이로 부터 회귀 분석이라는 용어가 쓰이게 되었다고 한다.\n",
    "    - 분류(Classification) : 결과가 숫자가 아니고 참, 거짓 같은 두 가지 혹은 여러 가지 종류로 구분하는 문제\n",
    "        - 출력 값의 여러 종류를 지칭할 때 레이블(label) 혹은 클래스(class)라고 부른다.\n",
    "    - 군집(Clustering) : 사전 정보 없이 쌓여 있는 그룹 정보를 의미 있는 서브그룹(subgroup) 또는 클러스터(cluster)로 조직하는 탐색적 데이터 분석 기법.\n",
    "        - 분석 과정에서 만든 각 클러스터는 어느 정도 유사성을 공유하고 다른 클러스터와는 비슷하지 않은 샘플 그룹을 형성 ==> 비지도 분류(unsupervised classification)\n",
    "        - 정보를 조직화하고 데이터에서 의미 있는 관계를 유도한다.\n",
    "    - 차원감소(Dimensionality Reduction) : 관련 있는 정보를 대부분 유지하면서 더 작은 차원의 부분 공간(subspace)으로 데이터를 압축\n",
    "        - 비지도 학습의 또 다른 하위 분야는 차원 축소(dimensionality reduction)\n",
    "        - 데이터 시각화에도 유용 (고차원 특성을 1차원 또는 2차원, 3차원 특성 공간으로 투영하여 3D와 2D 산점도(scatterplot)나 히스토그램(histogram)으로 시각화)\n",
    "- 최근 0.18.0 버전부터 인공 신경망(neural network)이 본격적으로 추가되기 시작 But, 위 그림엔 없다.\n",
    "- 데이터의 종류와 크기에 따라 어떤 알고리즘을 선택해야하는지 가이드 해주지만 반드시 이 치트 시트를 따르기보다 알고리즘을 선택하는 데 도움을 줄 수 있는 여러 가지 중 한 가지 참고 사항이 있다고 여기면 좋을듯하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NdjzA9w2fdE"
   },
   "source": [
    "![dimensionality reduction](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2F0iGzo%2FbtqClytctfi%2FGsCzCkE37SJcuGB4wynRM0%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8NPUmFU2fdF"
   },
   "source": [
    "# 04-7 사이킷런으로 로지스틱 회귀 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVx4OjqG2fdG"
   },
   "source": [
    "#### 사이킷런의 경사 하강법이 구현된 클래스 ==> SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyEjSwsa2fdH"
   },
   "source": [
    "- SGDClassifier(확률적 경사 하강법)\n",
    "    - 사이킷런 라이브러리에 있다.\n",
    "    - 로지스틱 회귀, 퍼셉트론, 서포트 벡터 머신(support vector machine, SVM, 기계 학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델이며 주로 분류와 회귀 분석을 위해 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUQMd4pZ2fdI"
   },
   "source": [
    "### 사이킷런으로 경사 하강법 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8jpDpG9g2fdJ"
   },
   "source": [
    "#### 1. 로지스틱 손실 함수 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2alFxV9I2fdK"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(loss='log', max_iter=100, tol=1e-3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eR_rOPt2fdM"
   },
   "source": [
    "|<center>매개변수</center>|<center>설명</center>|\n",
    "|:-------------|:-------------:|\n",
    "|<center>**loss**</center>|<center>손실 함수 지정</center>|\n",
    "|<center>**max_iter(iteration number)**</center>|<center>epochs 수 지정 (기본값 : 5)</center>|\n",
    "|<center>**tol**</center>|<center>지정한 값만큼 감소되지 않으면 반복 중단</center>|\n",
    "|<center>**random_state**</center>|<center>에포크만큼 반복 실행할때마다 사용할 데이터가 동일하게 재현되도록 하는 난수 초깃값</center>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Jj8m2VK2fdN"
   },
   "source": [
    "- 만약 tol 값을 설정하지 않으면? max_iter 값을 늘리라는 경고가 발생\n",
    "    - 모델의 로지스틱 손실 함수의 값이 최적값으로 수렴할 정도로 충분한 반복 횟수를 입력했는지 사용자에게 알려주므로 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opYhXdp72fdO"
   },
   "source": [
    "#### 2. 사이킷런으로 훈련하고 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYHJlqDj2fdR"
   },
   "source": [
    "- 사이킷런의 SGDClassifier 클래스에는 04-6에서 직접 구현한 메서드가 이미 준비되어있다. (사이킷런의 라이브러리...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "_Od1uDO72fdS",
    "outputId": "3d4a510e-09c3-4d9c-88f3-495650e2f6d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(x_train, y_train) # 사이킷런의 fit() 메서드로 훈련\n",
    "sgd.score(x_test, y_test) # 사이킷런의 score() 메서드로 정확도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTcjMC0f2fdU"
   },
   "source": [
    "#### 3. 사이킷런으로 예측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFHe2BKo2fdV"
   },
   "source": [
    "- 사이킷런은 입력 데이터로 2차원 배열만 받아들이므로 샘플 하나를 주입하더라도 2차원 배열로 만들어야 한다.\n",
    "    - 배열의 슬라이싱을 사용해 테스트 세트에서 10개의 샘플만 뽑아 예측을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Koj84neK2fdW",
    "outputId": "8d134f75-d670-4bee-b0c5-aebb0e2c3dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.predict(x_test[:10]) # 사이킷런의 predict() 메서드로 테스트 세트에 대한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "l_vWrllt2fdY",
    "outputId": "92a6449b-2e99-4bd9-9c3c-281d1bbb08d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "AbWq05pK2fda",
    "outputId": "6d55e806-f667-44ce-cf90-c0fb269fa830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sgd.predict(x_test[:10]) == y[:10]) # 예측 결과와 실제 타깃을 비교"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WPrfS_ov2fcO",
    "nvOuUftA2fcQ",
    "_rp9zSS82fcz",
    "EE97x2_A2fc6",
    "uNyxtRQM2fdC",
    "qVx4OjqG2fdG",
    "8jpDpG9g2fdJ",
    "opYhXdp72fdO",
    "yTcjMC0f2fdU"
   ],
   "name": "04-6, 04-7.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
