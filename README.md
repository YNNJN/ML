# ML



## Step 1 Tutorial

- 2020년 1월 19일  ~ 4월 18일(예정) 
- 'Do it! 정직하게 코딩하며 배우는 딥러닝 입문'의 튜토리얼을 따라 공부합니다.
- '밑바닥부터 시작하는 딥러닝', 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'Deep Learning Book', '파이토치 첫걸음'을 참고합니다.
- 위의 책들을 출처로 하여 학습 내용을 스스로 정리하고, 또한 구글링을 통해 자료를 보충하고 실습합니다.
- 이후 6명의 스터디원들과 함께 토론하고, 토론한 내용을 바탕으로 자료의 내용을 다듬습니다.
- 그 과정을 이곳에 기록합니다.





#### # Contents

---

#### 0. 사이킷런

**key words:** scikit-learn algorithm cheat-sheet, Classification, Regression, Clustering, Dimensionality Reduction

#### 1. 선형회귀 실습

**key words:** Linear Regression, Gradient Descent, Box plot, Epoch

#### 2. 손실함수와 활성화함수

**key words:** Loss function, Squared Error(SE), Mean Squared Error(MSE), Cross Entropy Error(CEE), Activation function, Sigmoid function, ReLU function, Softmax function

#### 3. 이진분류문제 접근

**key words:** Perceptron, XOR Gate, Adaline, Logistic Regression, Odds ratio, Logit function, Sigmoid function, Chain Rule, Backward Propagation

#### 4. 로지스틱회귀 실습

**key words:** Pandas DataFrame, Box plot, Splitting Data, Forward Propagation, Backward Propagation, Estimator Class

#### 5. 로지스틱회귀 실습2 (최적화)

**key words:** Hidden Layer, Bias, SGD(Stochastic Gradient Descent), BGD(Batch Gradient Descent), mini-BGD, SGD Classifier

#### 6. 훈련 노하우 (과적합 규제)

**key words:** KSVM(Kernel Support Vector Machine), Learning rate, Data Preprocessing, Overfitting, Underfitting, Regularization, Bias-Variance Trade-Off, Early stopping, L1 Regularization, L2 Regularization, Penalty, Sparse feature, Feature selection, Elastic Regularization, Data Augumentation, Normalization, Drop out, Batch Norm, k-Fold Cross-Validation, Pipeline

#### 7. 다층신경망 개념

**key words:** Tensor, Dot Product, StandardScaler, SGD(Stochastic Gradient Descent), BGD(Batch Gradient Descent), mini-BGD, MLP(Multilayer Perceptron), Fully-connected Neural Network, Regularization, Weight Initialization, MLPClassifier, MLPRegressor

#### 8. 다층신경망 실습 (Tensorflow, Keras)

**key words:** Multiclass Classification Neural Network, Softmax function, Cross Entropy Loss, One-Hot Encoding, LabelBinarizer, to_categorical, Tensorflow, Keras, Sequential, Dense, Optimizer

#### 9. CNN 구조와 개념

**key words:** Vanishing Gradient, ReLU, Optimizer, Adagrad, RMSProp, Momentum, Adam, Filter, Kernel, Feature map, Stride, Padding, Padding types (Valid, Same, Full), Channel, Max Pooling, Average Pooling, CNN Architecture

#### 10. CNN 실습

**key words:** Automatic differentiation, Convolution, ReLU, Max Pooling, Adam, Dropout